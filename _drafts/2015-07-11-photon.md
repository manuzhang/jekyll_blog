--------
layout: post
title: Photon: Fault-tolerant and Scalable Joining of Continuous Data Streams
--------

[Photon: Fault-tolerant and Scalable Joining of Continuous Data Streams](http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/41318.pdf) - Ananthanarayanan et al. 2013 


We are familar with how join is performed in a relational databases (RDBMS). A typical inner-join SQL could be 

```sql

select * from t_primary, t_foreign,
where t_primary.foreign_key = t_foreign.primary_key

```

A naive implementation of such a query is nested-loops-joins, where outer loop consumes table t_primary row by row and inner loop executed for each outer row searchs for matching rows in table t_foreign.

Joining two continuous data streams are like joining two tables in RDBMS but we are faced with far more challenges.

1. In RDBMS tables are pre-loaded while data streams are flowing into the system continuously and endlessly. We cannot wait for all the inputs being loaded.
2. Another reason we cannot wait is the latency requirements. Data streams are usually from real-time applications where a query must be anwser within a few seconds. Otherwise, the anwser would be valueless. Think about the case I asked about the traffic condition when driving home. If the system reported an hour later I would either already get home or be trapped in traffic jam !
3. It's possible that streams data are delayed or unordered. The arrival order of inputs with the same key from two data streams are uncertain. In the extreme case, it may take hours or days for one input finally has its match. Then it already fails the latency requirement.  
4. Inputs could be lost or resent. It's ok for the Facebook favorite counts but untolerable for Amazon's purchase system. Amazon doesn't want to lose money and users don't want to be charged twice. Hence, it's critical to ensure join is performed exactly-once.  
5. When the data volume cannot fit into a single machine we have to scale out and shard data on many commodity machines. For one, high scalability is required as data is ever growing. For another, network partitions become the norm and we are bounded by the [CAP theorem](http://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed). It means we have to choose either availablility over consistency or consistency over availability but not both.  

It's interesting to see how Google approaches these challenges in its joinning system, Photon, which is driving Google's Advertising System. Photon joins data streams such as web search queries and user clicks on advertisements and the joined log derives key business metrics includign billing for advertisers. It's geographically distributed and processes millions of events per minute at peak with an average end-to-end latency for less than 10 seconds. 

Here's how Photon joins a search query with subsequent clicks on ads.
![photon_join](https://lh3.googleusercontent.com/QOPFJROSqiVDRosE62TAFjjA_MGDMmkyEr8yD-eJLJ8=w879-h680-no)

Photon formalizes the problem as 

> Formally, given two continuously growing log streams such that each event in the primary log stream contains a unique identifier, and each event in the foreign log stream contains the identifier referring to an event in the primary log stream, we want to join each foreign log event with the corresponding primary log event and produce the joined event.

Photon face challenges described above. Additionally, Photon is required to automatically handle datacenter-level outage with no manual operations and no impact on system availability. Hence, there are at least two copies of Photon pipeline in differenct datacenters each of which continues processing independent of the other. 

Firstly, let's illustrate how a single Photon pipeline works.

![photon_pipeline](https://lh3.googleusercontent.com/d7ryPlWanPm-34Ok9JCs9xyUVyAowc5TiI7OnbbXkFg=w961-h606-no)

1. The dispatcher consumes the click events from the logs as they come in, and issues a lookup in the IdRegistry. If the click id already exists in the IdRegistry, the dispatcher assumes that the click has already been joined and skips processing the click.
2. If the click id does not exist in the IdRegistry, the dispatcher sends the click to the joiner asynchronously and waits for the response. If the joiner fails to join the click (say, due to a network problem, or because of a missing query event), the dispatcher will keep retrying by sending the click to another joiner instance after some backoff period. This guarantees at-least-once semantics with minimum losses.
3. The joiner extracts query id from the click and does a lookup in the EventStore to find the corresponding query.
4. If the query is not found, the joiner sends a failure response to the dispatcher so that it can retry. If the query is found, the joiner tries to register the click id into the IdRegistry.
5. If the click id already exists in the IdRegistry, the joiner assumes that the click has already been joined. If the joiner is able to register click id into the IdRegistry, the joiner stores information from the query in the click and writes the event to the joined click logs.

While the retrying logic guarantees at-least-once semantics, Photon relies on IdRegistry to coordinate output of Photon workers in different datacenters to guarantee that each input s joined at-most-once. IdRegistry is implemented built on an in-memory key-value store that is consistently replicated across multiple datacenters using [Paxos](http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf). Writing data which needs to be synchoronzied across wide-area network through Paxos is very expensive.  Hence, only meta-data is written to IdRegistry and such optimizations as server-side batching, timestamp-based dynamical sharding and deleting old keys are also implemented to improve throughput and scalability. For more details on IdRegistry and how Photon is fault-tolerant in other failure cases, please check the original paper. 

One more thing is that events deladyed by more than N days are discarded where  N is determined by evaluating the trade-off between the costage of storage of the cost of dropping such events.

This post is inspired by [The Morning Paper](http://blog.acolyer.org/).
