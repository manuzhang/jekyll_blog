---
author: manuzhang
comments: true
date: 2012-11-07 15:13:10+00:00
layout: post
published: false
slug: cassandra-partitioner
title: Cassandra - partitioning
wordpress_id: 773
categories:
- database
- 实习笔记
tags:
- Cassandra
---

Cassandra is **distributed**, **scalable** and **highly available**. To support those features, Cassandra implements a [Distributed Hash Table](http://en.wikipedia.org/wiki/Distributed_hash_table) using [Consistent Hashing](http://en.wikipedia.org/wiki/Consistent_hashing). Tables (for those from RDBMS world) in Cassandra are horizontally partitioned and replicated across the machines. Machines join and leave (called **churn**) with minimized data movements. Queries are routed to some replicas as long as they are not all down. 

In this post, I will talk about partitioning. Before that, let me briefly introduce the concepts of **Distributed Hash Table** and **Consistent Hashing**.

<!-- more -->



# Distributed Hash Table



As per [Wikipedia](http://en.wikipedia.org/wiki/Distributed_hash_table):


<blockquote>
A distributed hash table (DHT) is a class of a decentralized distributed system that provides a lookup service similar to a hash table; (key, value) pairs are stored in a DHT, and any participating node can efficiently retrieve the value associated with a given key.
</blockquote>



_Cassandra is de facto a key-value store_. Its (key, value) pair is [cci](RowKey, List)[/cci]. 

Unlike a hash table on a single machine, (key, value) pairs could be distributed across hundreds of machines (peers) in a DHT. Queries coming from any peer would be routed to the peer that stores the data. _In designing a DHT, there is trade off between the number of neighbors each peer has to track and the number of messages that the DHT needs to send to resolve a single query_. On one hand, if each peer tracks all other peers, then only one message is sent per query ([latex]O(1)[/latex] latency). On the other hand, if each peer is only aware of its predecessor and successor, [latex]N/2[/latex] messages are sent on average for each query([latex]O(n)[/latex] latency). Cassandra adopts the first approach to minimize latency.




# Consistent Hashing



With a classical hash table (using the linear congruential function [latex]x \mapsto a * x + b \pmod{p}[/latex]), the churn of one peer will cause nearly all keys to be remapped. This is undesirable because change in ownership typically corresponds to bandwidth-intensive movement of objects stored in the DHT from one node to another. To mitigate this effect, most DHTs use some variant of [Consistent Hashing](http://en.wikipedia.org/wiki/Consistent_hashing) to map keys to nodes. We will talk about one variant here.


Imagine the universe of consistent hashing as a ring below:



![ring](https://lh4.googleusercontent.com/-lhHVlVR8i1c/UJou5XHk5nI/AAAAAAAAAuQ/90LizK2EZT4/w360-h205-n-k/Screenshot%2Bfrom%2B2012-11-07%2B17%253A49%253A46.png)



The positions on the ring are a set of big integers (generated by a hash function), with the maximum value wrapping around to the minimum value. Unlike traditional hash tables, both nodes and keys are ordinary citizens living at their mapped positions on the ring, as in the above graph. 

**# So at which node will we store "key" ?** 
We walk clockwise along the ring and we store "key" at the first node we find. In this case, B. 
**# What if a new node D joins the ring and is mapped to the range between "key" and B? **
Applying the same rule, we move "key" from B to D and the other nodes are left unaffected.
**# What if B is scheduled to be removed from the ring?**
As in previous cases, we move "key" from B to C. Again, the other nodes stay unaffected.
 


# Partitioning in Cassandra




In Cassandra, that big integer is wrapped as a Token, or rather, a BigIntegerToken, which is generated by a hash function (MD5, SHA-1, etc). Each node gets a random Token or is manually assigned a unique Token, which determines what keys it is the **first replica** for (if all nodes' Tokens are sorted, a node is the first replica for [cci](PreviousToken, MyToken][/cci], which is the same rule we've applied earlier).

Token generation is dispatched to a **Partitioner**. For instance, _RandomPartitioner_ uses [MD5](http://en.wikipedia.org/wiki/MD5). Unfortunately, MD5 is not collision resistent. Hence, a Token corresponds to a range of keys. Despite that, the correctness of mapping from keys to nodes holds as long as each key gets a unique Token. A more accurate abstract would be **DecoratedKey**, a [cci][/cci] pair. For the ranges on the ring, there are two types of abstractions, _Bounds_ and _Range_ both extending the abstract class **AbstractBounds**. If we denote a range by [cci][/cci], the difference is that _Bounds_ contains both endpoints while _Range_ excludes left endpoint. _Bounds_ is used by-key range scans while _Range_ is used by the partitioner and by map/reduce by-token range scans.

The relationship is illustrated in the UML class diagram below (I'm still struggling with the clarity provided by lucidchart).



![token](https://lh5.googleusercontent.com/-14C-Xl575Vc/UJsXMwuGRXI/AAAAAAAAAuk/Jac9Kx-O6QQ/w320-h167-n-k/Screenshot%2Bfrom%2B2012-11-08%2B10%253A20%253A20.png)



Here's a full list of Partitioners provided by Cassandra (configurable in [cci]conf/cassandra.yaml [/cci]).





  * RandomPartitioner distributes rows across the cluster evenly by md5.
    **When in doubt, this is the best option.**


  * Murmur3Partitioner is similar to RandomPartioner but uses [Murmur3_128 Hash Function](https://code.google.com/p/smhasher/) instead of md5 (default since 1.2)


  * ByteOrderedPartitioner orders rows lexically by key bytes.  BOP allows scanning rows in key order, but the ordering can generate hot spots for sequential insertion workloads.


  * OrderPreservingPartitioner is an obsolete form of BOP, that stores keys in a less-efficient format and only works with keys that are UTF8-encoded Strings.


  * CollatingOPP collates according to EN,US rules rather than lexical byte ordering.  Use this as an example if you need custom collation.



_Note that Partitioner is configured per-node so all the nodes in a cluster should have a consistent Partitioner_. 



# Virtual Nodes


Although with Consistent Hashing we avoid key remapping, a cluster will still become unbalanced when a new host(physical node) is added. In order for the cluster to be balanced, we could double the number of hosts when scaling up and half the number when scaling down or move the tokens of all the hosts.

_Cassandra 1.2_ makes use of virtual nodes to address the issue, where each host is responsible for multiple, non-contiguous ranges of the data. The post [Virtual Nodes Strategies](http://www.acunu.com/2/post/2012/07/virtual-nodes-strategies.html) discusses the motivation, benefits and schemes to bring virtual node support into Cassandra. More information could be found at [CASSANDRA-4119](https://issues.apache.org/jira/browse/CASSANDRA-4119).

The number of tokens a host is responsible for can be specified in [cci]conf/cassandra.yaml[/cci]
[cc lang="yaml"]
# This defines the number of tokens randomly assigned to this node on the ring
# The more tokens, relative to other nodes, the larger the proportion of data
# that this node will store. You probably want all nodes to have the same number
# of tokens assuming they have equal hardware capability.
#
# If you leave this unspecified, Cassandra will use the default of 1 token for legacy compatibility,
# and will use the initial_token as described below.
#
# Specifying initial_token will override this setting.
#
# If you already have a cluster with 1 token per node, and wish to migrate to 
# multiple tokens per node, see http://wiki.apache.org/cassandra/Operations
# num_tokens: 1
[/cc]

The benefits of using virtual nodes (mostly from that post):




  * A host joining the cluster could take data evenly from each other host; 
   Likewise, a host leaving the cluster could transfer data evenly to each other host


  * If a host fails, the load will be spread evenly across other hosts in the cluster; 
   Rebuilding a dead host will be faster as it will involve every other host 


  * The number of partitions assigned to a host can be determined by its capacity, allowing for heterogeneity in the hardware. 



That being said, there are some concerns over [outage increases](http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/Virtual-Nodes-lots-of-physical-nodes-and-potentially-increasing-outage-count-td7584135.html) since a host now involves multiple ranges of a cluster. The post [Improving Cassandra's uptime with virtual nodes](http://www.acunu.com/2/post/2012/10/improving-cassandras-uptime-with-virtual-nodes.html) argues that the rebuild time is sufficiently faster so that the total cluster outage time will actually be lower. 

Virtual nodes are actually described in the [Amazon Dynamo paper](http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf). It is unclear to me why virtual nodes were not implemented in Cassandra in the first place since Cassandra combines the best of Dynamo (Data partition) and [BigTable](http://en.wikipedia.org/wiki/Bigtable) (Data model).




# Conclusion


Cassandra horizontally partition data with consistent hashing across virtual nodes. 




#  References 






  1. http://wiki.apache.org/cassandra/Partitioners


  2. http://www.datastax.com/docs/1.1/cluster_architecture/partitioning


  3. [Karger, D.; Lehman, E.; Leighton, T.; Panigrahy, R.; Levine, M.; Lewin, D. (1997). "Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web". Proceedings of the Twenty-ninth Annual ACM Symposium on Theory of Computing. ACM Press New York, NY, USA. pp. 654–663.](http://dl.acm.org/citation.cfm?id=258660)

